{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights matrix  and biases vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(100, 784), (10, 100)] and [(100, 1), (10, 1)]\n"
     ]
    }
   ],
   "source": [
    "sizes = [784, 100, 10]\n",
    "\n",
    "num_layers = len(sizes)\n",
    "\n",
    "biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "\n",
    "print([w.shape for w in weights], 'and', [b.shape for b in biases])\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0 + np.exp(-z))\n",
    "\n",
    "\n",
    "def feedforward(a):\n",
    "    for b, w in zip(biases, weights):\n",
    "        a = sigmoid(np.dot(w, a) + b)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(100, 784), (10, 100)] and [(100, 1), (10, 1)]\n"
     ]
    }
   ],
   "source": [
    "print([w.shape for w in weights], 'and', [b.shape for b in biases])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 1)\n"
     ]
    }
   ],
   "source": [
    "inv = np.zeros((784, 1))\n",
    "\n",
    "res = feedforward(inv)\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.37, 0.35, 0.00, 0.95, 0.03, 0.99, 0.15, 0.99, 1.00, 0.09]\n"
     ]
    }
   ],
   "source": [
    "print(np.array2string(res.reshape((-1,)), separator=', ', formatter={'float_kind': lambda x: f'{x:0.2f}'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    with gzip.open('data/mnist.pkl.gz', 'rb') as f:\n",
    "        train_data, valid_data, test_data = pickle.load(f, encoding='latin1')\n",
    "    \n",
    "    return train_data, valid_data, test_data\n",
    "\n",
    "\n",
    "train_data, valid_data, test_data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(50000, 784), (50000,)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[t.shape for t in train_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = train_data\n",
    "x_valid, y_valid = valid_data\n",
    "x_test, y_test = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 8, 4, 8])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_categorical(x):\n",
    "    num_categories = np.unique(x).shape[0]\n",
    "    vector = np.eye(num_categories, dtype='uint8')[x]\n",
    "    return vector.reshape((-1, 10, 1))\n",
    "\n",
    "yy_train = to_categorical(y_train)\n",
    "yy_valid = to_categorical(y_valid)\n",
    "yy_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yy_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]] 5\n"
     ]
    }
   ],
   "source": [
    "print(yy_train[0], y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx_train = x_train.reshape((-1, 784, 1))\n",
    "xx_valid = x_valid.reshape((-1, 784, 1))\n",
    "xx_test = x_test.reshape((-1, 784, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = np.random.randint(x_train.shape[0])\n",
    "plt.imshow(x_train[idx].reshape((28, 28)))\n",
    "plt.title(f'digit: {y_train[idx]}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 0 0 0 0 0 0]] 3\n"
     ]
    }
   ],
   "source": [
    "idx = np.random.randint(yy_train.shape[0])\n",
    "\n",
    "print(yy_train[idx].transpose(), y_train[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD\n",
    "Train the neural network using mini-batch stochastic gradient descent.  The `training_data` is a list of tuples\n",
    "`(x, y)` representing the training inputs and the desired outputs.  The other non-optional parameters are self-explanatory.  If `test_data` is provided then the network will be evaluated against the test data after each epoch, and partial progress printed out.  This is useful for tracking progress, but slows things down substantially.\n",
    "\n",
    "## update_mini_batch\n",
    "Update the network's weights and biases by applying gradient descent using backpropagation to a single mini batch. The `mini_batch` is a list of tuples `(x, y)`, and `eta` is the learning rate.\n",
    "\n",
    "## backdrop\n",
    "Return a tuple `(nabla_b, nabla_w)` representing the gradient for the cost function C_x.  `nabla_b` and `nabla_w` are layer-by-layer lists of numpy arrays, similar to `self.biases` and `self.weights`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SGD(x, y, epochs, mini_batch_size, eta, test_data=None):\n",
    "    training_data = np.array(list(zip(x, y)))\n",
    "    n = training_data.shape[0]\n",
    "    \n",
    "    if test_data is not None:\n",
    "        tx, ty = test_data\n",
    "        n_test = tx.shape[0]\n",
    "    \n",
    "    for j in range(epochs):\n",
    "        np.random.shuffle(training_data)\n",
    "        \n",
    "        mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
    "        \n",
    "        for mini_batch in mini_batches:\n",
    "            update_mini_batch(mini_batch, eta)\n",
    "            \n",
    "        loss, acc, match, total = evaluate(x, y)\n",
    "        print(f'Training dataset: {match} / {total},  loss: {loss:.7f},  accuracy: {acc:.2f}%')\n",
    "            \n",
    "        if test_data is not None:          \n",
    "            loss, acc, match, total = evaluate(tx, ty)\n",
    "            print(\n",
    "                f'Epoch {j:02d}: test dataset {match} / {total} ', \n",
    "                f'test loss: {loss:.7f},  test accuracy: {acc:.2f}%'\n",
    "            )\n",
    "        else:\n",
    "            print(f'Epoch {j:02d} complete')\n",
    "\n",
    "\n",
    "def update_mini_batch(mini_batch, eta):\n",
    "    global biases, weights\n",
    "    \n",
    "    nabla_b = [np.zeros(b.shape) for b in biases]\n",
    "    nabla_w = [np.zeros(w.shape) for w in weights]\n",
    "\n",
    "    for x, y in mini_batch:\n",
    "        delta_nabla_b, delta_nabla_w = backprop(x, y)\n",
    "        nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "        nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "\n",
    "    n_batch = len(mini_batch)\n",
    "    \n",
    "    weights = [w-eta/n_batch*nw for w, nw in zip(weights, nabla_w)]\n",
    "    biases  = [b-eta/n_batch*nb for b, nb in zip(biases,  nabla_b)]\n",
    "\n",
    "\n",
    "def backprop(x, y):\n",
    "    nabla_b = [np.zeros(b.shape) for b in biases]\n",
    "    nabla_w = [np.zeros(w.shape) for w in weights]\n",
    "    \n",
    "    # feed forward\n",
    "    activation = x\n",
    "    \n",
    "    activations = [x] # list to store all the activations, layer by layer\n",
    "    zs = [] # list to store all the z vectors, layer by layer\n",
    "    \n",
    "    for b, w in zip(biases, weights):\n",
    "        z = np.dot(w, activation) + b\n",
    "        zs.append(z)\n",
    "        activation = sigmoid(z)\n",
    "        activations.append(activation)\n",
    "        \n",
    "    # backward pass\n",
    "    \n",
    "    # cost dericative\n",
    "    delta = (activations[-1] - y) * sigmoid_prime(zs[-1])\n",
    "    \n",
    "    nabla_b[-1] = delta\n",
    "    nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "    \n",
    "    for l in range(2, num_layers):\n",
    "        delta = np.dot(weights[-l+1].transpose(), delta) * sigmoid_prime(zs[-l])\n",
    "        \n",
    "        nabla_b[-l] = delta\n",
    "        nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        \n",
    "    return nabla_b, nabla_w\n",
    "        \n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "\n",
    "def evaluate(x, y):\n",
    "    x = np.array([feedforward(l) for l in x])\n",
    "    \n",
    "    num_x = x.shape[0]\n",
    "    \n",
    "    loss = np.sum(np.linalg.norm(x-y)) / (2. * num_x)\n",
    "    \n",
    "    x = np.argmax(x, axis=1).reshape((-1,))\n",
    "    y = np.argmax(y, axis=1).reshape((-1,))\n",
    "    \n",
    "    match = np.sum(np.int8(x == y))\n",
    "    \n",
    "    acc = match / num_x * 100\n",
    "\n",
    "    return loss, acc, match, num_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset: 44842 / 50000,  loss: 0.0007369,  accuracy: 89.68%\n",
      "Epoch 00: test dataset 8738 / 10000  test loss: 0.0020152,  test accuracy: 87.38%\n",
      "Training dataset: 44837 / 50000,  loss: 0.0007369,  accuracy: 89.67%\n",
      "Epoch 01: test dataset 8739 / 10000  test loss: 0.0020137,  test accuracy: 87.39%\n"
     ]
    }
   ],
   "source": [
    "SGD(x=xx_train, y=yy_train, epochs=30, mini_batch_size=10, eta=3.0, test_data=(xx_test, yy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation dataset: 8766 / 10000,  loss: 0.00201,  accuracy: 87.66%\n"
     ]
    }
   ],
   "source": [
    "loss, acc, match, total = evaluate(xx_valid, yy_valid)\n",
    "\n",
    "print(f'validation dataset: {match} / {total},  loss: {loss:.5f},  accuracy: {acc:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset: 44819 / 50000,  loss: 0.0007372,  accuracy: 89.64%\n",
      "Epoch 00: test dataset 8744 / 10000  test loss: 0.0020136,  test accuracy: 87.44%\n",
      "Training dataset: 44814 / 50000,  loss: 0.0007372,  accuracy: 89.63%\n",
      "Epoch 01: test dataset 8740 / 10000  test loss: 0.0020138,  test accuracy: 87.40%\n",
      "Training dataset: 44823 / 50000,  loss: 0.0007371,  accuracy: 89.65%\n",
      "Epoch 02: test dataset 8747 / 10000  test loss: 0.0020142,  test accuracy: 87.47%\n",
      "Training dataset: 44799 / 50000,  loss: 0.0007372,  accuracy: 89.60%\n",
      "Epoch 03: test dataset 8738 / 10000  test loss: 0.0020158,  test accuracy: 87.38%\n",
      "Training dataset: 44823 / 50000,  loss: 0.0007371,  accuracy: 89.65%\n",
      "Epoch 04: test dataset 8745 / 10000  test loss: 0.0020140,  test accuracy: 87.45%\n",
      "Training dataset: 44827 / 50000,  loss: 0.0007371,  accuracy: 89.65%\n",
      "Epoch 05: test dataset 8745 / 10000  test loss: 0.0020136,  test accuracy: 87.45%\n",
      "Training dataset: 44819 / 50000,  loss: 0.0007370,  accuracy: 89.64%\n",
      "Epoch 06: test dataset 8747 / 10000  test loss: 0.0020138,  test accuracy: 87.47%\n",
      "Training dataset: 44834 / 50000,  loss: 0.0007370,  accuracy: 89.67%\n",
      "Epoch 07: test dataset 8748 / 10000  test loss: 0.0020136,  test accuracy: 87.48%\n",
      "Training dataset: 44859 / 50000,  loss: 0.0007369,  accuracy: 89.72%\n",
      "Epoch 08: test dataset 8750 / 10000  test loss: 0.0020131,  test accuracy: 87.50%\n",
      "Training dataset: 44850 / 50000,  loss: 0.0007369,  accuracy: 89.70%\n",
      "Epoch 09: test dataset 8746 / 10000  test loss: 0.0020133,  test accuracy: 87.46%\n"
     ]
    }
   ],
   "source": [
    "SGD(x=xx_train, y=yy_train, epochs=10, mini_batch_size=10, eta=3.0, test_data=(xx_test, yy_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('dnn-weights-nb.pkl.gz', 'wb') as f:\n",
    "    model_weights = (biases, weights)\n",
    "    f.write(pickle.dumps(model_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
