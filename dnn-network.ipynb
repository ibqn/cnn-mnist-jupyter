{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dnn-network.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQ1ZUEBKFIRM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "d3b09da0-2703-4845-e538-b01c655c20c6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd '/gdrive/My Drive/cnn-mnist-jupyter/'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive/My Drive/cnn-mnist-jupyter\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbYbfKtFE2Wi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import pickle\n",
        "import gzip\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMQofxO1E2XN",
        "colab_type": "text"
      },
      "source": [
        "# Weights matrix  and biases vector"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaL_vIKkE2XR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9eedffb8-3baa-4b69-d6a0-6f3ef0c7677f"
      },
      "source": [
        "sizes = [784, 100, 10]\n",
        "\n",
        "num_layers = len(sizes)\n",
        "\n",
        "biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "weights = [np.random.randn(y, x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "\n",
        "print([w.shape for w in weights], 'and', [b.shape for b in biases])\n",
        "\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1.0/(1.0 + np.exp(-z))\n",
        "\n",
        "\n",
        "def feedforward(a):\n",
        "    for b, w in zip(biases, weights):\n",
        "        a = sigmoid(np.dot(w, a) + b)\n",
        "    return a"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(100, 784), (10, 100)] and [(100, 1), (10, 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaVhIsGWE2Xx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "84d72a76-dc75-4dec-a63a-db872dada918"
      },
      "source": [
        "print([w.shape for w in weights], 'and', [b.shape for b in biases])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(100, 784), (10, 100)] and [(100, 1), (10, 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJe-qRXlE2Yb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "340a0202-3baa-4d2c-a2dc-e398a35e74b8"
      },
      "source": [
        "inv = np.zeros((784, 1))\n",
        "\n",
        "res = feedforward(inv)\n",
        "print(res.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(10, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdmpI3hoE2Yy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1690bf37-a769-4e6a-a79d-9e5ec81fd75a"
      },
      "source": [
        "print(np.array2string(res.reshape((-1,)), separator=', ', formatter={'float_kind': lambda x: f'{x:0.2f}'}))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.00, 1.00, 0.77, 0.17, 0.98, 1.00, 0.33, 0.00, 0.01, 1.00]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJ3BMx2kE2ZG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data():\n",
        "    with gzip.open('data/mnist.pkl.gz', 'rb') as f:\n",
        "        train_data, valid_data, test_data = pickle.load(f, encoding='latin1')\n",
        "    \n",
        "    return train_data, valid_data, test_data\n",
        "\n",
        "\n",
        "train_data, valid_data, test_data = load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kf0WErw8E2Zv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "501949c8-58d2-4863-ca76-c6a8f39c82d3"
      },
      "source": [
        "[t.shape for t in train_data]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(50000, 784), (50000,)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4ehLSidE2Z_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, y_train = train_data\n",
        "x_valid, y_valid = valid_data\n",
        "x_test, y_test = test_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_LX6ZEBE2aG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3836992b-3b24-42b2-ded9-b84560f3217a"
      },
      "source": [
        "y_train"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, ..., 8, 4, 8])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8BTZ0twE2av",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def to_categorical(x):\n",
        "    num_categories = np.unique(x).shape[0]\n",
        "    vector = np.eye(num_categories, dtype='uint8')[x]\n",
        "    return vector.reshape((-1, 10, 1))\n",
        "\n",
        "yy_train = to_categorical(y_train)\n",
        "yy_valid = to_categorical(y_valid)\n",
        "yy_test = to_categorical(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sl-SR0-oE2bI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "61808080-cc21-4e48-9201-afac6221fe39"
      },
      "source": [
        "yy_train.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 10, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-FjjtwjE2bX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "55eb9ed0-1bff-48cf-868f-6970a9bba499"
      },
      "source": [
        "print(yy_train[0], y_train[0])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]] 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBmWnD1rE2dj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d1597f82-ba13-4cf2-b15c-b72c926bf1bc"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 784)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XV-KaGLaE2eG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xx_train = x_train.reshape((-1, 784, 1))\n",
        "xx_valid = x_valid.reshape((-1, 784, 1))\n",
        "xx_test = x_test.reshape((-1, 784, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6Ldw6e2E2ef",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "c92b409c-9e4a-414a-879f-15f09e14de44"
      },
      "source": [
        "idx = np.random.randint(x_train.shape[0])\n",
        "plt.imshow(x_train[idx].reshape((28, 28)))\n",
        "plt.title(f'digit: {y_train[idx]}')\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAEICAYAAACQ6CLfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nN2EL5PjnH0YAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tsFwJU8BE2e5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2c32777b-88d1-4a91-d471-ee5f873f44fd"
      },
      "source": [
        "idx = np.random.randint(yy_train.shape[0])\n",
        "\n",
        "print(yy_train[idx].transpose(), y_train[idx])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0 0 0 0 0 0 0 1 0]] 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3H97y9x5E2fI",
        "colab_type": "text"
      },
      "source": [
        "## SGD\n",
        "Train the neural network using mini-batch stochastic gradient descent.  The `training_data` is a list of tuples\n",
        "`(x, y)` representing the training inputs and the desired outputs.  The other non-optional parameters are self-explanatory.  If `test_data` is provided then the network will be evaluated against the test data after each epoch, and partial progress printed out.  This is useful for tracking progress, but slows things down substantially.\n",
        "\n",
        "## update_mini_batch\n",
        "Update the network's weights and biases by applying gradient descent using backpropagation to a single mini batch. The `mini_batch` is a list of tuples `(x, y)`, and `eta` is the learning rate.\n",
        "\n",
        "## backdrop\n",
        "Return a tuple `(nabla_b, nabla_w)` representing the gradient for the cost function C_x.  `nabla_b` and `nabla_w` are layer-by-layer lists of numpy arrays, similar to `self.biases` and `self.weights`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMWIB9wnE2fM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def SGD(x, y, epochs, mini_batch_size, eta, test_data=None):\n",
        "    training_data = np.array(list(zip(x, y)))\n",
        "    n = training_data.shape[0]\n",
        "    \n",
        "    if test_data is not None:\n",
        "        tx, ty = test_data\n",
        "        n_test = tx.shape[0]\n",
        "    \n",
        "    for j in range(epochs):\n",
        "        np.random.shuffle(training_data)\n",
        "        \n",
        "        mini_batches = [training_data[k:k+mini_batch_size] for k in range(0, n, mini_batch_size)]\n",
        "        \n",
        "        for mini_batch in mini_batches:\n",
        "            update_mini_batch(mini_batch, eta)\n",
        "            \n",
        "        loss, acc, match, total = evaluate(x, y)\n",
        "        print(f'Training dataset: {match} / {total},  loss: {loss:.7f},  accuracy: {acc:.2f}%')\n",
        "            \n",
        "        if test_data is not None:          \n",
        "            loss, acc, match, total = evaluate(tx, ty)\n",
        "            print(\n",
        "                f'Epoch {j:02d}: test dataset {match} / {total} ', \n",
        "                f'test loss: {loss:.7f},  test accuracy: {acc:.2f}%'\n",
        "            )\n",
        "        else:\n",
        "            print(f'Epoch {j:02d} complete')\n",
        "\n",
        "\n",
        "def update_mini_batch(mini_batch, eta):\n",
        "    global biases, weights\n",
        "    \n",
        "    nabla_b = [np.zeros(b.shape) for b in biases]\n",
        "    nabla_w = [np.zeros(w.shape) for w in weights]\n",
        "\n",
        "    for x, y in mini_batch:\n",
        "        delta_nabla_b, delta_nabla_w = backprop(x, y)\n",
        "        nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "        nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "\n",
        "    n_batch = len(mini_batch)\n",
        "    \n",
        "    weights = [w-eta/n_batch*nw for w, nw in zip(weights, nabla_w)]\n",
        "    biases  = [b-eta/n_batch*nb for b, nb in zip(biases,  nabla_b)]\n",
        "\n",
        "\n",
        "def backprop(x, y):\n",
        "    nabla_b = [np.zeros(b.shape) for b in biases]\n",
        "    nabla_w = [np.zeros(w.shape) for w in weights]\n",
        "    \n",
        "    # feed forward\n",
        "    activation = x\n",
        "    \n",
        "    activations = [x] # list to store all the activations, layer by layer\n",
        "    zs = [] # list to store all the z vectors, layer by layer\n",
        "    \n",
        "    for b, w in zip(biases, weights):\n",
        "        z = np.dot(w, activation) + b\n",
        "        zs.append(z)\n",
        "        activation = sigmoid(z)\n",
        "        activations.append(activation)\n",
        "        \n",
        "    # backward pass\n",
        "    \n",
        "    # cost dericative\n",
        "    delta = (activations[-1] - y) * sigmoid_prime(zs[-1])\n",
        "    \n",
        "    nabla_b[-1] = delta\n",
        "    nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
        "    \n",
        "    for l in range(2, num_layers):\n",
        "        delta = np.dot(weights[-l+1].transpose(), delta) * sigmoid_prime(zs[-l])\n",
        "        \n",
        "        nabla_b[-l] = delta\n",
        "        nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
        "        \n",
        "    return nabla_b, nabla_w\n",
        "        \n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    s = sigmoid(z)\n",
        "    return s * (1 - s)\n",
        "\n",
        "\n",
        "def evaluate(x, y):\n",
        "    x = np.array([feedforward(l) for l in x])\n",
        "    \n",
        "    num_x = x.shape[0]\n",
        "    \n",
        "    loss = np.sum(np.linalg.norm(x-y)**2) / (2. * num_x)\n",
        "    \n",
        "    x = np.argmax(x, axis=1).reshape((-1,))\n",
        "    y = np.argmax(y, axis=1).reshape((-1,))\n",
        "    \n",
        "    match = np.sum(np.int8(x == y))\n",
        "    \n",
        "    acc = match / num_x * 100\n",
        "\n",
        "    return loss, acc, match, num_x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "lBiLC4_pE2fT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1229631f-e17d-4ba5-f989-f5a4b69ebaa3"
      },
      "source": [
        "SGD(x=xx_train, y=yy_train, epochs=40, mini_batch_size=10, eta=3.0, test_data=(xx_test, yy_test))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training dataset: 36202 / 50000,  loss: 0.1744866,  accuracy: 72.40%\n",
            "Epoch 00: test dataset 7294 / 10000  test loss: 0.1733435,  test accuracy: 72.94%\n",
            "Training dataset: 42020 / 50000,  loss: 0.1056560,  accuracy: 84.04%\n",
            "Epoch 01: test dataset 8378 / 10000  test loss: 0.1085547,  test accuracy: 83.78%\n",
            "Training dataset: 42682 / 50000,  loss: 0.0951941,  accuracy: 85.36%\n",
            "Epoch 02: test dataset 8520 / 10000  test loss: 0.0996447,  test accuracy: 85.20%\n",
            "Training dataset: 43011 / 50000,  loss: 0.0906334,  accuracy: 86.02%\n",
            "Epoch 03: test dataset 8535 / 10000  test loss: 0.0965145,  test accuracy: 85.35%\n",
            "Training dataset: 43307 / 50000,  loss: 0.0852025,  accuracy: 86.61%\n",
            "Epoch 04: test dataset 8550 / 10000  test loss: 0.0942722,  test accuracy: 85.50%\n",
            "Training dataset: 47691 / 50000,  loss: 0.0505029,  accuracy: 95.38%\n",
            "Epoch 05: test dataset 9443 / 10000  test loss: 0.0584058,  test accuracy: 94.43%\n",
            "Training dataset: 48255 / 50000,  loss: 0.0312834,  accuracy: 96.51%\n",
            "Epoch 06: test dataset 9559 / 10000  test loss: 0.0400107,  test accuracy: 95.59%\n",
            "Training dataset: 48475 / 50000,  loss: 0.0279975,  accuracy: 96.95%\n",
            "Epoch 07: test dataset 9561 / 10000  test loss: 0.0392852,  test accuracy: 95.61%\n",
            "Training dataset: 48579 / 50000,  loss: 0.0249324,  accuracy: 97.16%\n",
            "Epoch 08: test dataset 9565 / 10000  test loss: 0.0382277,  test accuracy: 95.65%\n",
            "Training dataset: 48726 / 50000,  loss: 0.0228482,  accuracy: 97.45%\n",
            "Epoch 09: test dataset 9593 / 10000  test loss: 0.0366809,  test accuracy: 95.93%\n",
            "Training dataset: 48760 / 50000,  loss: 0.0224433,  accuracy: 97.52%\n",
            "Epoch 10: test dataset 9569 / 10000  test loss: 0.0385384,  test accuracy: 95.69%\n",
            "Training dataset: 48842 / 50000,  loss: 0.0198713,  accuracy: 97.68%\n",
            "Epoch 11: test dataset 9611 / 10000  test loss: 0.0366236,  test accuracy: 96.11%\n",
            "Training dataset: 48934 / 50000,  loss: 0.0187706,  accuracy: 97.87%\n",
            "Epoch 12: test dataset 9611 / 10000  test loss: 0.0358944,  test accuracy: 96.11%\n",
            "Training dataset: 48985 / 50000,  loss: 0.0179402,  accuracy: 97.97%\n",
            "Epoch 13: test dataset 9615 / 10000  test loss: 0.0357338,  test accuracy: 96.15%\n",
            "Training dataset: 48971 / 50000,  loss: 0.0180754,  accuracy: 97.94%\n",
            "Epoch 14: test dataset 9597 / 10000  test loss: 0.0369355,  test accuracy: 95.97%\n",
            "Training dataset: 49071 / 50000,  loss: 0.0161460,  accuracy: 98.14%\n",
            "Epoch 15: test dataset 9602 / 10000  test loss: 0.0354537,  test accuracy: 96.02%\n",
            "Training dataset: 49109 / 50000,  loss: 0.0148236,  accuracy: 98.22%\n",
            "Epoch 16: test dataset 9622 / 10000  test loss: 0.0353011,  test accuracy: 96.22%\n",
            "Training dataset: 49137 / 50000,  loss: 0.0151588,  accuracy: 98.27%\n",
            "Epoch 17: test dataset 9605 / 10000  test loss: 0.0364965,  test accuracy: 96.05%\n",
            "Training dataset: 49180 / 50000,  loss: 0.0128767,  accuracy: 98.36%\n",
            "Epoch 18: test dataset 9624 / 10000  test loss: 0.0340216,  test accuracy: 96.24%\n",
            "Training dataset: 49184 / 50000,  loss: 0.0134759,  accuracy: 98.37%\n",
            "Epoch 19: test dataset 9612 / 10000  test loss: 0.0354248,  test accuracy: 96.12%\n",
            "Training dataset: 49203 / 50000,  loss: 0.0126369,  accuracy: 98.41%\n",
            "Epoch 20: test dataset 9618 / 10000  test loss: 0.0348991,  test accuracy: 96.18%\n",
            "Training dataset: 49246 / 50000,  loss: 0.0116348,  accuracy: 98.49%\n",
            "Epoch 21: test dataset 9637 / 10000  test loss: 0.0337734,  test accuracy: 96.37%\n",
            "Training dataset: 49261 / 50000,  loss: 0.0108956,  accuracy: 98.52%\n",
            "Epoch 22: test dataset 9611 / 10000  test loss: 0.0345436,  test accuracy: 96.11%\n",
            "Training dataset: 49270 / 50000,  loss: 0.0106020,  accuracy: 98.54%\n",
            "Epoch 23: test dataset 9636 / 10000  test loss: 0.0338558,  test accuracy: 96.36%\n",
            "Training dataset: 49279 / 50000,  loss: 0.0103680,  accuracy: 98.56%\n",
            "Epoch 24: test dataset 9624 / 10000  test loss: 0.0348297,  test accuracy: 96.24%\n",
            "Training dataset: 49292 / 50000,  loss: 0.0099205,  accuracy: 98.58%\n",
            "Epoch 25: test dataset 9626 / 10000  test loss: 0.0337345,  test accuracy: 96.26%\n",
            "Training dataset: 49307 / 50000,  loss: 0.0096706,  accuracy: 98.61%\n",
            "Epoch 26: test dataset 9637 / 10000  test loss: 0.0338231,  test accuracy: 96.37%\n",
            "Training dataset: 49316 / 50000,  loss: 0.0090407,  accuracy: 98.63%\n",
            "Epoch 27: test dataset 9637 / 10000  test loss: 0.0336972,  test accuracy: 96.37%\n",
            "Training dataset: 49328 / 50000,  loss: 0.0089596,  accuracy: 98.66%\n",
            "Epoch 28: test dataset 9634 / 10000  test loss: 0.0336802,  test accuracy: 96.34%\n",
            "Training dataset: 49335 / 50000,  loss: 0.0087086,  accuracy: 98.67%\n",
            "Epoch 29: test dataset 9640 / 10000  test loss: 0.0329654,  test accuracy: 96.40%\n",
            "Training dataset: 49338 / 50000,  loss: 0.0086085,  accuracy: 98.68%\n",
            "Epoch 30: test dataset 9625 / 10000  test loss: 0.0335224,  test accuracy: 96.25%\n",
            "Training dataset: 49342 / 50000,  loss: 0.0083684,  accuracy: 98.68%\n",
            "Epoch 31: test dataset 9633 / 10000  test loss: 0.0332397,  test accuracy: 96.33%\n",
            "Training dataset: 49351 / 50000,  loss: 0.0082750,  accuracy: 98.70%\n",
            "Epoch 32: test dataset 9638 / 10000  test loss: 0.0336154,  test accuracy: 96.38%\n",
            "Training dataset: 49348 / 50000,  loss: 0.0082294,  accuracy: 98.70%\n",
            "Epoch 33: test dataset 9634 / 10000  test loss: 0.0330526,  test accuracy: 96.34%\n",
            "Training dataset: 49360 / 50000,  loss: 0.0080629,  accuracy: 98.72%\n",
            "Epoch 34: test dataset 9630 / 10000  test loss: 0.0331987,  test accuracy: 96.30%\n",
            "Training dataset: 49366 / 50000,  loss: 0.0079705,  accuracy: 98.73%\n",
            "Epoch 35: test dataset 9636 / 10000  test loss: 0.0333715,  test accuracy: 96.36%\n",
            "Training dataset: 49369 / 50000,  loss: 0.0079828,  accuracy: 98.74%\n",
            "Epoch 36: test dataset 9631 / 10000  test loss: 0.0339137,  test accuracy: 96.31%\n",
            "Training dataset: 49372 / 50000,  loss: 0.0078392,  accuracy: 98.74%\n",
            "Epoch 37: test dataset 9631 / 10000  test loss: 0.0332287,  test accuracy: 96.31%\n",
            "Training dataset: 49382 / 50000,  loss: 0.0077586,  accuracy: 98.76%\n",
            "Epoch 38: test dataset 9642 / 10000  test loss: 0.0331762,  test accuracy: 96.42%\n",
            "Training dataset: 49379 / 50000,  loss: 0.0076520,  accuracy: 98.76%\n",
            "Epoch 39: test dataset 9641 / 10000  test loss: 0.0331390,  test accuracy: 96.41%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvfMmYM0E2fq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "db160e14-8b07-465b-b561-3dd21bebbdb7"
      },
      "source": [
        "loss, acc, match, total = evaluate(xx_valid, yy_valid)\n",
        "\n",
        "print(f'validation dataset: {match} / {total},  loss: {loss:.5f},  accuracy: {acc:.2f}%')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "validation dataset: 9672 / 10000,  loss: 0.03214,  accuracy: 96.72%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cbfmbR9E2gv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with gzip.open('dnn-weights-nb.pkl.gz', 'wb') as f:\n",
        "    model_weights = (biases, weights)\n",
        "    f.write(pickle.dumps(model_weights))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nx9RXwFXE2hF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}